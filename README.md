<!-- <h1 align="center">Inferential Question Answering (Inferential QA)</h1> -->

<p align="center">
  <img src="asset/quit_logo.svg" alt="Inferential QA Logo" width="400"/>
</p>

<p align="center">
  <a href="https://huggingface.co/datasets/JamshidJDMY/InferentialQA"><img src="https://img.shields.io/static/v1?label=Dataset&message=HuggingFace&color=yellow&logo=huggingface"></a>
  <a href=""><img src="https://img.shields.io/static/v1?label=Paper&message=Unpublished&color=green&logo=arXiv"></a>
  <a href="https://opensource.org/license/apache-2-0"><img src="https://img.shields.io/static/v1?label=License&message=MIT&color=red"></a>
</p>

# Inferential Question Answering (Inferential QA)

**Inferential Question Answering (Inferential QA)** introduces a new class of reasoning QA tasks that challenge models to infer answers from indirect textual evidence rather than extracting them directly from answer-containing passages.

We present **QUIT (QUestions requiring Inference from Texts)** â€” a large-scale benchmark of **7,401 questions** and **2.4 million passages**, designed to evaluate how well modern retrieval-augmented systems and large language models (LLMs) can perform inference-based reasoning.



## ðŸ§  Motivation

Most existing QA datasets assume *answer containment* â€” that the answer explicitly appears in a retrieved passage.  
However, many real-world questions (e.g., educational reasoning, knowledge-based inference) require deriving answers from **clues and context** instead.

Inferential QA bridges this gap by focusing on **answer-supporting passages** â€” those that provide **evidence for inference**, not the answer itself.



## ðŸ“˜ QUIT: A Benchmark for Inferential QA

**QUIT (QUestions requiring Inference from Texts)** is a **large-scale benchmark** designed to test whether modern QA systems can solve questions where:

âœ… the evidence is present  
âŒ but the answer is *not explicitly stated*  

Unlike traditional QA datasets, QUIT focuses on **answer-supporting passages**: passages that contain **clues**, not spans.

### ðŸ”¥ Benchmark Highlights

- ðŸ§  **7,401 inference-heavy questions**
- ðŸ“š **2.4M passages** built from compositional hint combinations
- ðŸ§© Each question has **325 candidate passages**
- ðŸŽ¯ Multi-level relevance labels:
  - **2**: fully relevant (enables inference)
  - **1**: partially relevant (weak or indirect evidence)
  - **0**: irrelevant

### ðŸ“Š Benchmark Statistics

| Split     | # Questions |    # Passages |
| :-------- | ----------: | ------------: |
| Train     |       4,811 |     1,563,575 |
| Dev       |         862 |       280,150 |
| Test      |       1,728 |       561,600 |
| **Total** |   **7,401** | **2,405,325** |



## ðŸ“¦ Dataset Access (Download QUIT)

âœ… The full QUIT benchmark is publicly available on HuggingFace:

ðŸ‘‰ **HuggingFace Dataset:** https://huggingface.co/datasets/JamshidJDMY/InferentialQA

### ðŸš€ Quick Downloads

- **ðŸ“¥ Corpus (2.4M passages)**  
  https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/corpus/corpus.jsonl?download=true

- **ðŸ“¥ Train Set (4,811 questions)**  
  https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/train.json?download=true

- **ðŸ“¥ Dev Set (862 questions)**  
  https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/dev.json?download=true

- **ðŸ“¥ Test Set (1,728 questions)**  
  https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/test.json?download=true


### âš¡ Recommended Usage

- Use the **Corpus** for indexing (retrievers / rerankers)
- Use **Train** for fine-tuning retrievers/rerankers
- Use **Dev/Test** for *fair comparison* and reporting benchmark numbers



## âš™ï¸ Methodology

**QUIT** is constructed in two stages:

### 1. Question Sampling

- Source datasets: **TriviaHG** (machine-authored hints) & **WikiHint** (human-authored hints)
- Filtered using **BEM** to remove answer leakage
- Question type and difficulty estimated via **HintEval**
- Removed questions that LLMs could answer *parametrically* (without context)

### 2. Dataset Preparation

- Generated all subsets and permutations of top-5 hints per question â†’ **325 passages per question**
- Labeled using **Gemma 3 1B**, **Qwen 3 4B**, **LLaMA 3.1 8B** with GPT-Eval
- Dev/Test verified by human annotators and relabeled for leakage



## ðŸ§© Experimental Setup

We evaluate a **Retrieverâ€“Rerankerâ€“Reader** pipeline across multiple models:

| Component          | Models                              |
| :----------------- | :---------------------------------- |
| **Retrievers**     | BM25, DPR, ColBERT, Contriever, BGE |
| **Rerankers**      | LiT5, MonoT5, RankGPT, RankT5, UPR  |
| **Readers (LLMs)** | LLaMA 3.2 1B, Gemma 3 4B, Qwen 3 8B |

Evaluation metrics: **Hit@K**, **Recall@K**, **MRR**, **NDCG@K**, and **Exact Match (EM)**.

### ðŸ“Œ Key Observation

If retrieval and reranking were perfect, LLMs could achieve **â‰ˆ 90% EM (oracle)**.  
However, current pipelines reach only **~10â€“15% EM**.

General-purpose LLMs (**Gemma 3 4B**) outperform reasoning-oriented ones (**Qwen 3 8B**), showing that scaling or reasoning orientation alone does not solve inference-based QA.



## ðŸ” Overall Insights

- ðŸ§­ **Retrieval** is the dominant bottleneck â€” current retrievers cannot locate answer-supporting passages.
- ðŸ” **Reranking** helps little; fine-tuning retrievers and rerankers gives inconsistent gains.
- ðŸ§  **General-purpose LLMs** (e.g., Gemma 3 4B) handle inferential reasoning better than reasoning-specialized ones.
- ðŸš¨ The gap between **Oracle (~90% EM)** and **real pipelines (~10%)** exposes the core limitation of todayâ€™s RAG systems in inference-based reasoning.



## ðŸ’» Reproducibility & Evaluation

We release QUIT together with **full reproducibility scripts** and **pre-computed results**, so anyone can:

âœ… reproduce all benchmark numbers  
âœ… evaluate new retrievers / rerankers / readers  
âœ… compare against strong baselines  



### ðŸ› ï¸ Option A â€” Reproduce Everything From Scratch

> âš ï¸ Recommended: **Python 3.10** (some dependencies are not fully compatible with newer versions)

```bash
git clone https://github.com/DataScienceUIBK/InferentialQA.git
cd InferentialQA
pip install -r requirements.txt
```

All experiments are organized inside `experiments/`.  
To reproduce any experiment:

1. go to its folder  
2. run the provided `run.sh`  

âœ… Suggested order (end-to-end benchmark reproduction):

- `experiments/dataset`  
  Download QUIT from HuggingFace

- `experiments/index`  
  Build indexes and preprocess corpus

- `experiments/baseline`  
  Wikipedia / MSMARCO baselines

- `experiments/vanilla/oracle-rerankers`  
  Oracle reranker experiments (upper-bound analysis)

- `experiments/vanilla/retrievers`  
  Retriever-only benchmark runs

- `experiments/vanilla/rerankers`  
  Retriever + reranker

- `experiments/vanilla/rag`  
  Full Retriever â†’ Reranker â†’ Reader pipeline



### ðŸ”¥ Fine-tuning Experiments (Optional)

We also provide scripts to fine-tune components on QUIT:

- `experiments/finetuning/colbert`
- `experiments/finetuning/dpr`
- `experiments/finetuning/monot5`

And complete pipeline evaluations:

- `experiments/finetuning_pipeline/ft-retriever/reranker`
- `experiments/finetuning_pipeline/ft-retriever/rag`
- `experiments/finetuning_pipeline/ft-reranker/retriever`
- `experiments/finetuning_pipeline/ft-reranker/rag`
- `experiments/finetuning_pipeline/ft-reranker/retriever_reranker`

> âš¡ Note: some fine-tuning experiments require serious compute  
> e.g., **â‰¥ 1Ã— NVIDIA A100 GPU**, and can take **multiple days**.

### ðŸ§° Additional Environments (Required for Some Fine-tuning)

Some fine-tuning pipelines rely on external toolkits. Please set up their environments separately:

- **ColBERT (using & fine-tuning)**: follow the official repository: https://github.com/stanford-futuredata/ColBERT
- **DPR fine-tuning**: use **Tevatron** and follow their instructions: https://github.com/texttron/tevatron
- **MonoT5 fine-tuning**: use **pygaggle** and follow their instructions: https://github.com/castorini/pygaggle



### âœ… Option B â€” Use Our Precomputed Results (No GPU Needed)

No powerful resources? No problem.

We provide **precomputed outputs** for all benchmark experiments.
To reproduce tables and analysis from the paper:

1. go to the `results/` directory  
2. run the Python scripts

They will automatically download the needed files from HuggingFace and display the final results.

ðŸŽ‰ This option makes QUIT easy to use for:
- quick benchmarking
- ablation studies
- comparing new models
- classroom/educational usage



## ðŸ† Leaderboard (Coming Soon)

| Rank |   Model   | Retriever |  Reranker   |    Reader     |   EM   |
| :--: | :-------: | :-------: | :---------: | :-----------: | :----: |
|  â­  | Optimal   |     â€“     |      â€“      |  Gemma-3-4B   | 90.16% |
|  ðŸ¥‡  | Baseline  |    BGE    |   MonoT5    |  Gemma-3-4B   | 15.34% |
|  ðŸ¥ˆ  | Baseline  |    BGE    |  FT-MonoT5  |  Gemma-3-4B   | 13.89% |
|  ðŸ¥‰  | Baseline  |    BGE    |      â€“      |  Gemma-3-4B   | 13.18% |


Stay tuned for the **official leaderboard** and evaluation scripts once the dataset is released.



## ðŸš€ Key Takeaways

- ðŸ” **Inferential QA** requires reasoning from clues â€” not explicit spans
- âš™ï¸ **Current retrievers and rerankers** fail to identify sufficient evidence
- ðŸ§© **Fine-tuning** is insufficient; new paradigms for *retrieval-augmented reasoning* are needed
- ðŸ“ˆ **QUIT** exposes a fundamental limitation in todayâ€™s QA pipelines and opens a new research direction



## ðŸ“Œ Citation

If you use **InferentialQA / QUIT** in your research, please cite our paper:

```bibtex
```

> We will update this BibTeX entry once the paper is officially published.



## ðŸ“„ License

This project is released under the **MIT License**. See the [LICENSE](LICENSE) file for details.


## ðŸš€ Contribution Summary

âœ… Introduce **Inferential QA**, a new reasoning-based QA task  
âœ… Construct **QUIT**, the first large-scale dataset for inferential question answering  
âœ… Evaluate **retrievers**, **rerankers**, and **LLM readers** extensively  
âœ… Show that current QA pipelines fail under inference-based reasoning  
