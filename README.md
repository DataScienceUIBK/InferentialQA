<!-- <h1 align="center">Inferential Question Answering (Inferential QA)</h1> -->

<p align="center">
  <img src="asset/quit_logo.svg" alt="Inferential QA Logo" width="400"/>
</p>

<p align="center">
  <a href="https://huggingface.co/datasets/JamshidJDMY/InferentialQA"><img src="https://img.shields.io/static/v1?label=Dataset&message=HuggingFace&color=yellow&logo=huggingface"></a>
  <a href=""><img src="https://img.shields.io/static/v1?label=Paper&message=Unpublished&color=green&logo=arXiv"></a>
  <a href="https://opensource.org/license/apache-2-0"><img src="https://img.shields.io/static/v1?label=License&message=MIT&color=red"></a>
</p>

**Inferential Question Answering (Inferential QA)** introduces a new class of reasoning QA tasks that challenge models to infer answers from indirect textual evidence rather than extracting them directly from answer-containing passages.

We present **QUIT (QUestions requiring Inference from Texts)** â€” a large-scale benchmark of **7,401 questions** and **2.4 million passages**, designed to evaluate how well modern retrieval-augmented systems and large language models (LLMs) can perform inference-based reasoning.

## ğŸ§  Motivation

Most existing QA datasets assume *answer containment* â€” that the answer explicitly appears in a retrieved passage.
However, many real-world questions (e.g., educational reasoning, knowledge-based inference) require deriving answers from **clues and context** instead.

Inferential QA bridges this gap by focusing on **answer-supporting** passages â€” those that provide **evidence for inference**, not the answer itself.

## ğŸ“˜ The QUIT Dataset

The **QUIT** dataset consists of passages built from *hints* â€” short, human- or machine-authored clues describing entities without revealing their names.

| Split     | # Questions |    # Passages |
| :-------- | ----------: | ------------: |
| Train     |       4,811 |     1,563,575 |
| Dev       |         862 |       280,150 |
| Test      |       1,728 |       561,600 |
| **Total** |   **7,401** | **2,405,325** |

Each passage is labeled at **three relevance levels**:

* **2 â€“ Fully relevant:** enables an LLM to infer the correct answer
* **1 â€“ Partially relevant:** indirectly describes the answer
* **0 â€“ Irrelevant:** unrelated to the answer

## ğŸ“¦ Dataset Access

You can download the QUIT dataset from the following links:

* [ğŸ“¥ Corpus](https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/corpus/corpus.jsonl?download=true)
* [ğŸ“¥ Train Set](https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/train.json?download=true)
* [ğŸ“¥ Dev Set](https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/dev.json?download=true)
* [ğŸ“¥ Test Set](https://huggingface.co/datasets/JamshidJDMY/InferentialQA/resolve/main/test.json?download=true)

## âš™ï¸ Methodology

**QUIT** is constructed in two stages:

### 1. Question Sampling

* Source datasets: **TriviaHG** (machine-authored hints) & **WikiHint** (human-authored hints).
* Filtered using **BEM** to remove answer leakage.
* Question type and difficulty estimated via **HintEval**.
* Removed questions that LLMs could answer *parametrically* (without context).

### 2. Dataset Preparation

* Generated all subsets and permutations of top-5 hints per question â†’ 325 passages per question.
* Labeled using **Gemma 3 1B**, **Qwen 3 4B**, **LLaMA 3.1 8B** with GPT-Eval.
* Dev/Test verified by human annotators and relabeled for leakage.

## ğŸ§© Experimental Setup

We evaluate a **Retrieverâ€“Rerankerâ€“Reader** pipeline across multiple models:

| Component          | Models                              |
| :----------------- | :---------------------------------- |
| **Retrievers**     | BM25, DPR, ColBERT, Contriever, BGE |
| **Rerankers**      | LiT5, MonoT5, RankGPT, RankT5, UPR  |
| **Readers (LLMs)** | LLaMA 3.2 1B, Gemma 3 4B, Qwen 3 8B |

Evaluation metrics: **Hit@K**, **Recall@K**, **MRR**, **NDCG@K**, and **Exact Match (EM)**.

**ğŸ§© Observation:**
If retrieval and reranking were perfect, LLMs could achieve â‰ˆ 90 % EM (oracle).
Current pipelines reach only ~10â€“15 %. General-purpose LLMs (**Gemma 3 4B**) outperform reasoning-oriented ones (**Qwen 3 8B**), showing that scaling or reasoning orientation alone does not solve inference-based QA.


### **Overall Insights**

* ğŸ§­ **Retrieval** is the dominant bottleneck â€” current retrievers cannot locate answer-supporting passages.
* ğŸ” **Reranking** helps little; fine-tuning retrievers and rerankers gives inconsistent gains.
* ğŸ§  **General-purpose LLMs** (e.g., Gemma 3 4B) handle inferential reasoning better than reasoning-specialized ones.
* ğŸš¨ The gap between **Oracle (~90 % EM)** and **real pipelines (~10 %)** exposes the core limitation of todayâ€™s RAG systems in inference-based reasoning.

## ğŸ’» Code & Evaluation (Coming Soon)

To reproduce results and evaluate on QUIT:

```bash
git clone https://github.com/DataScienceUIBK/inferential-qa.git
cd inferential-qa
pip install -r requirements.txt
python evaluate.py --model bge --reranker monot5 --reader gemma
```

Evaluation script supports:

* Custom retrievers, rerankers, or LLM readers
* Both zero-shot and fine-tuned evaluation
* Metrics: *Hit@K, Recall@K, MRR, NDCG@K, EM*


## ğŸ† Leaderboard (Coming Soon)

| Rank | Model | Retriever | Reranker | Reader |  EM | NDCG@10 |
| :--: | :---- | :-------- | :------- | :----- | :-: | :-----: |
|  ğŸ¥‡  | â€“     | â€“         | â€“        | â€“      |  â€“  |    â€“    |
|  ğŸ¥ˆ  | â€“     | â€“         | â€“        | â€“      |  â€“  |    â€“    |
|  ğŸ¥‰  | â€“     | â€“         | â€“        | â€“      |  â€“  |    â€“    |

Stay tuned for the **official leaderboard** and evaluation scripts once the dataset is released.


## ğŸš€ Key Takeaways

* ğŸ” **Inferential QA** requires reasoning from clues â€” not explicit spans.
* âš™ï¸ **Current retrievers and rerankers** fail to identify sufficient evidence.
* ğŸ§© **Fine-tuning** is insufficient; new paradigms for *retrieval-augmented reasoning* are needed.
* ğŸ“ˆ **QUIT** exposes a fundamental limitation in todayâ€™s QA pipelines and opens a new research direction.

## ğŸš€ Contribution Summary

âœ… Introduce **Inferential QA**, a new reasoning-based QA task.  
âœ… Construct **QUIT**, the first large-scale dataset for inferential question answering.  
âœ… Evaluate **retrievers**, **rerankers**, and **LLM readers** extensively.  
âœ… Show that current QA pipelines fail under inference-based reasoning.  

